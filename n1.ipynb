{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JahnaviKumar8/voice-based-emotion-recognition-using-Transformers/blob/main/n1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfKhQlY0TFAZ",
        "outputId": "5c8b7fdb-91c2-416c-a917-b168e0266d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "ERROR: unknown command \"install¬†accelerate¬†-U\"\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "!pip install¬†accelerate¬†-U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXgm_weC8RaW",
        "outputId": "e82539a0-6b99-4080-abad-5cad81a165b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AomcxDRYTS_w",
        "outputId": "f5e4c9d0-8d94-4593-e67a-8894b0b84377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install datasets\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATOKJ0yrSqAA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8lnkFDLSwsK"
      },
      "outputs": [],
      "source": [
        "def load_audio_files(audio_path, target_sr=16000, max_duration=35):\n",
        "    audio_data = []\n",
        "    labels = []\n",
        "\n",
        "    e_f = os.listdir(audio_path)\n",
        "\n",
        "    for emotion in  e_f:\n",
        "        e_p = os.path.join(audio_path, emotion)\n",
        "        audio_files = os.listdir(e_p)\n",
        "\n",
        "        for audio_file in audio_files[:10]:  # Limit to 10 files per singer for now\n",
        "            file_path = os.path.join(e_p, audio_file)\n",
        "\n",
        "            # Load and resample audio\n",
        "            audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
        "\n",
        "            # Normalize\n",
        "            audio = audio.astype(np.float32) / np.max(np.abs(audio))\n",
        "\n",
        "            audio_data.append(audio)\n",
        "            labels.append(emotion)\n",
        "\n",
        "    return np.array(audio_data, dtype=object), np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noHrOKNzT_PI",
        "outputId": "af1bbd91-407e-41a5-da02-e9513e496566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5OR5AEETrwG",
        "outputId": "231d446c-3a07-4bca-c710-4d9e7d5b18cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-5-bf9155bc7a30>:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(file_path, sr=target_sr, duration=max_duration)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels (emotion names): ['clips_angry' 'clips_happy' 'clips_sad']\n"
          ]
        }
      ],
      "source": [
        "audio_path = '/content/drive/MyDrive/ColabDrive/total'\n",
        "audio_data, labels = load_audio_files(audio_path)\n",
        "\n",
        "unique_labels = np.unique(labels)\n",
        "print(\"labels (emotion names):\", unique_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv61NczyUJ6U"
      },
      "outputs": [],
      "source": [
        "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "id_to_label = {i: label for label, i in label_to_id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "9c489ebda9194633bb19e66e4018a57e",
            "b58b0eb12c1f443dbed654fe8d734c70",
            "362ace8f10e144eb90c574111bd2106d",
            "6d7430f37f304fa2be532f9a09c6aadf",
            "1c6b3e2fdc244e22b55265fb622004f2",
            "f4e0c09e42a14cad93e8f91554d4f7b7",
            "edceff8ff4494a13bbc3c74fecd73f6a",
            "09616d5fdf954ca4af87dfb4d612db09",
            "235593f5380b4ac7afc43949b0b0af8a",
            "d4db3f96dd914460899491123f8066ca",
            "7be18603adfe4fdfa0feb4c5d6d502b5",
            "b702b3dae7e14505a0a4d285e7dea0be",
            "9ba797f8005742b2a7334d50afe0e11a",
            "37f5a6639e2e496297b98f011ac22089",
            "9871f60daec84b04a3774684f2723c77",
            "76b46386841941bab0e7a5b4baacdf74",
            "db045aa0e872404584e5a6a746f11344",
            "6357fa642afe41ed96894c5a156e09da",
            "4652ac5ea67b44e181e66bbbd8bd7b1b",
            "a563cb9bf11c456bbea6d57979bbc54a",
            "7e8fd1b6b6644eb78f88ba66c733b524",
            "9edcb389d90c4d2a97bbb28e3c176c15",
            "0a03a68036944285a67d4be98d57149c",
            "f977f66658b64094a218bceb327d246d",
            "3d787af8d766451591d83db4dbf63ce3",
            "62ad80147f6f43bf9a6592027db827d0",
            "16e822cb96fb44afb71b83fe210eb388",
            "9676fc96641740a0ae385f384e19bea0",
            "520fda3ad70f4d16bb90e41338139843",
            "40676458d90b4b56890904e59b8c3661",
            "4360b3e286f54fccb119e7dcf908f405",
            "ce24995d61b4445082ab2cb56dbc4d8d",
            "eb4d1bd56ca34cafa45bda1f058cd169"
          ]
        },
        "id": "PHxPvY-oUTJ6",
        "outputId": "c6afd200-6570-4950-b97b-41326fb38821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c489ebda9194633bb19e66e4018a57e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b702b3dae7e14505a0a4d285e7dea0be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a03a68036944285a67d4be98d57149c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "label_ids = np.array([label_to_id[label] for label in labels])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "    audio_data, label_ids, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = Dataset.from_dict({\"input_values\": train_data.tolist(), \"label\": train_labels.tolist()})\n",
        "val_dataset = Dataset.from_dict({\"input_values\": val_data.tolist(), \"label\": val_labels.tolist()})\n",
        "\n",
        "# Initialize feature extractor and model for audio classification\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "classification_model = AutoModelForAudioClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    num_labels=len(unique_labels),\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Define the data collator\n",
        "def data_collator(features):\n",
        "    input_features = [feature[\"input_values\"] for feature in features]\n",
        "    label_ids = [feature[\"label\"] for feature in features]\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        input_features,\n",
        "        sampling_rate=16000,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    inputs[\"labels\"] = torch.tensor(label_ids)\n",
        "    return inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKvrsxaNUVWW",
        "outputId": "e1685ddb-facf-46ac-ada5-5adfaed39510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=100,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,  # Enable mixed precision training for GPU\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    dataloader_num_workers=2,\n",
        "    max_grad_norm=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUqdyzPqUZno"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=classification_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=feature_extractor,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovgvHM1IKwsN"
      },
      "outputs": [],
      "source": [
        "# Define the data collator\n",
        "def data_collator(features):\n",
        "    input_features = [feature[\"input_values\"] for feature in features]\n",
        "    label_ids = [feature[\"label\"] for feature in features]\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        input_features,\n",
        "        sampling_rate=16000,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Ensure labels are converted to float32\n",
        "    inputs[\"labels\"] = torch.tensor(label_ids, dtype=torch.float32)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-AKQZSQTUly-",
        "outputId": "a2a8046e-aedf-4742-ef98-1432cf04ddde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 15:19, Epoch 66/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.102214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.102214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.102051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.101725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.100260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.098958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.095378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.090658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.084147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.077474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.070638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.060872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.053792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.041667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.029704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.014486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.995280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.971029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.944987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.926839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.898193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.868490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.839193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.814372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.791382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.771200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.754313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.742228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.733602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.708008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.661987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.588460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.523315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.429708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "\n",
        "def test_multiple_samples(model, feature_extractor, audio_path, id_to_label, num_samples=30):\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for emotion in os.listdir(audio_path):\n",
        "        e_p = os.path.join(audio_path, emotion)\n",
        "        audio_files = os.listdir(e_p)\n",
        "\n",
        "        for _ in range(num_samples // len(os.listdir(audio_path))):\n",
        "            audio_file = random.choice(audio_files)\n",
        "            file_path = os.path.join(e_p, audio_file)\n",
        "\n",
        "            audio, _ = librosa.load(file_path, sr=16000, duration=35)\n",
        "            inputs = feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to('cuda').half() for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(**inputs).logits\n",
        "            predicted_class_id = logits.argmax().item()\n",
        "            predicted_label = id_to_label[predicted_class_id]\n",
        "\n",
        "            predictions.append(predicted_label)\n",
        "            true_labels.append(emotion)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return predictions, true_labels, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NAgGnKHXhnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b385b2-75c7-4b68-8f63-01d089b900dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "<ipython-input-14-1d0eb698224a>:34: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction distribution: Counter({'clips_sad': 13, 'clips_happy': 10, 'clips_angry': 7})\n",
            "True label distribution: Counter({'clips_angry': 10, 'clips_happy': 10, 'clips_sad': 10})\n",
            "Accuracy: 0.8333333333333334\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " clips_angry       0.86      0.60      0.71        10\n",
            " clips_happy       1.00      1.00      1.00        10\n",
            "   clips_sad       0.69      0.90      0.78        10\n",
            "\n",
            "    accuracy                           0.83        30\n",
            "   macro avg       0.85      0.83      0.83        30\n",
            "weighted avg       0.85      0.83      0.83        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "import librosa\n",
        "import random\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save_model(\"./trained_audio_classifier\")\n",
        "\n",
        "# Move model to the appropriate device and convert to half precision if using GPU\n",
        "classification_model = classification_model.to(device)\n",
        "if device.type == 'cuda':\n",
        "    classification_model = classification_model.half()\n",
        "\n",
        "# Function to run multi-sample test\n",
        "def test_multiple_samples(model, feature_extractor, audio_path, id_to_label, num_samples=30):\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for emotion in os.listdir(audio_path):\n",
        "        emotion_path = os.path.join(audio_path, emotion)\n",
        "        audio_files = os.listdir(emotion_path)\n",
        "\n",
        "        for _ in range(num_samples // len(os.listdir(audio_path))):\n",
        "            audio_file = random.choice(audio_files)\n",
        "            file_path = os.path.join(emotion_path, audio_file)\n",
        "\n",
        "            audio, _ = librosa.load(file_path, sr=16000, duration=5)\n",
        "            inputs = feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(device).half() if device.type == 'cuda' else v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(**inputs).logits\n",
        "            predicted_class_id = logits.argmax().item()\n",
        "            predicted_label = id_to_label[predicted_class_id]\n",
        "\n",
        "            predictions.append(predicted_label)\n",
        "            true_labels.append(emotion)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return predictions, true_labels, accuracy # Added return statement\n",
        "\n",
        "\n",
        "# Run multi-sample test\n",
        "predictions, true_labels, accuracy = test_multiple_samples(classification_model, feature_extractor, audio_path, id_to_label)\n",
        "\n",
        "# Print results\n",
        "print(\"Prediction distribution:\", Counter(predictions))\n",
        "print(\"True label distribution:\", Counter(true_labels))\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxE3qx6CYAmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e433d40-52e9-40ea-879a-be9fca4d4457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. You can use your GPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. You can use your GPU.\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU instead.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G5aeZAVtFfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9127c3d-5eab-4a5a-8d09-ffb5ad7bcbf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-8f59b7457536>:1: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  test_audio, _ = librosa.load(os.path.join(audio_path, unique_labels[0], os.listdir(os.path.join(audio_path, unique_labels[0]))[0]), sr=16000, duration=5)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        }
      ],
      "source": [
        "test_audio, _ = librosa.load(os.path.join(audio_path, unique_labels[0], os.listdir(os.path.join(audio_path, unique_labels[0]))[0]), sr=16000, duration=5)\n",
        "test_input = feature_extractor(test_audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "test_input = {k: v.to('cuda').half() for k, v in test_input.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB7_U1s7EkBk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "# Save the label mapping\n",
        "label_map_path = os.path.join(\"./trained_audio_classifier\", \"label_map.json\")\n",
        "with open(label_map_path, \"w\") as f:\n",
        "    json.dump(id_to_label, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7bZI9smtNA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c4044d-72ed-4646-db6c-aaded7a74a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted emotion for single sample: clips_angry\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    logits = classification_model(**test_input).logits\n",
        "predicted_class_id = logits.argmax().item()\n",
        "predicted_label = id_to_label[predicted_class_id]\n",
        "print(f\"\\nPredicted emotion for single sample: {predicted_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdLc-qz72sbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa71b659-bb41-4252-e478-5e285c74802c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Single Sample Test ---\n",
            "\n",
            "Testing on a single sample:\n",
            "Selected singer: clips_angry\n",
            "Selected audio file: New Project - Made with Clipchamp (3).mp4\n",
            "\n",
            "Predicted singer: clips_angry\n",
            "Confidence: 65.42%\n",
            "Actual singer: clips_angry\n",
            "\n",
            "Top 3 predictions:\n",
            "clips_angry: 65.42%\n",
            "clips_sad: 21.68%\n",
            "clips_happy: 12.89%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "\n",
        "# Test the model on a single sample\n",
        "def test_single_sample(model, feature_extractor, audio_path, id_to_label, unique_labels):\n",
        "    # Randomly select a singer and a song\n",
        "    test_singer = random.choice(unique_labels)\n",
        "    test_singer_path = os.path.join(audio_path, test_singer)\n",
        "    test_audio_file = random.choice(os.listdir(test_singer_path))\n",
        "    test_audio_path = os.path.join(test_singer_path, test_audio_file)\n",
        "\n",
        "    print(f\"\\nTesting on a single sample:\")\n",
        "    print(f\"Selected singer: {test_singer}\")\n",
        "    print(f\"Selected audio file: {test_audio_file}\")\n",
        "\n",
        "    # Load and preprocess the audio\n",
        "    waveform, sample_rate = torchaudio.load(test_audio_path)\n",
        "    if sample_rate != 16000:\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Trim or pad to 20 seconds\n",
        "    if waveform.shape[1] > 20 * 16000:\n",
        "        waveform = waveform[:, :20 * 16000]\n",
        "    else:\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, 20 * 16000 - waveform.shape[1]))\n",
        "\n",
        "    waveform = waveform.squeeze().numpy()\n",
        "\n",
        "    # Prepare input for the model\n",
        "    test_input = feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=\"max_length\", max_length=int(16000 * 20), truncation=True)\n",
        "    test_input = {k: v.to('cuda').half() for k, v in test_input.items()}  # Move input to GPU and convert to half precision\n",
        "\n",
        "    # Get model prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(**test_input)\n",
        "        logits = output.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get predicted class and probability\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    predicted_label = id_to_label[predicted_class_id]\n",
        "    predicted_probability = probabilities[0][predicted_class_id].item()\n",
        "\n",
        "    print(f\"\\nPredicted singer: {predicted_label}\")\n",
        "    print(f\"Confidence: {predicted_probability:.2%}\")\n",
        "    print(f\"Actual singer: {test_singer}\")\n",
        "    print(\"\\nTop 3 predictions:\")\n",
        "\n",
        "    # Get top 3 predictions\n",
        "    top3_prob, top3_indices = torch.topk(probabilities, 3)\n",
        "    for i in range(3):\n",
        "        print(f\"{id_to_label[top3_indices[0][i].item()]}: {top3_prob[0][i].item():.2%}\")\n",
        "\n",
        "    return predicted_label, test_singer\n",
        "\n",
        "# Add this to your main code after training and before the multi-sample test\n",
        "print(\"\\n--- Single Sample Test ---\")\n",
        "predicted_label, actual_label = test_single_sample(classification_model, feature_extractor, audio_path, id_to_label, unique_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMhxQbhf2ySy"
      },
      "outputs": [],
      "source": [
        " import json\n",
        "\n",
        "# Save the label mapping\n",
        "label_map_path = os.path.join(\"./trained_audio_classifier\", \"label_map.json\")\n",
        "with open(label_map_path, \"w\") as f:\n",
        "    json.dump(id_to_label, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er9I24NI21hk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162fb27c-d2b0-4fa3-9939-52fd3302e874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification¬†complete.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Load your trained model and feature extractor\n",
        "model_name = \"./trained_audio_classifier\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "# Load the label mapping\n",
        "label_map_path = os.path.join(model_name, \"label_map.json\")\n",
        "with open(label_map_path, \"r\") as f:\n",
        "    id_to_label = json.load(f)\n",
        "\n",
        "# Function to preprocess and predict\n",
        "def predict_singer(audio_file_path):\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "    # Resample if necessary (assuming your model expects 16kHz)\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Convert to mono if stereo\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Preprocess the audio\n",
        "    inputs = feature_extractor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    # Get the predicted class\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    predicted_singer = id_to_label[str(predicted_class_id)]\n",
        "\n",
        "    return predicted_singer\n",
        "\n",
        "# Directory containing the input MP3 files\n",
        "input_directory = \"/content/drive/MyDrive/INPUT\"\n",
        "\n",
        "# Get all MP3 files in the input directory\n",
        "mp3_files = [f for f in os.listdir(input_directory) if f.endswith('.mp3')]\n",
        "\n",
        "# Process each MP3 file\n",
        "for mp3_file in mp3_files:\n",
        "    audio_file_path = os.path.join(input_directory, mp3_file)\n",
        "    try:\n",
        "        predicted_singer = predict_singer(audio_file_path)\n",
        "        print(f\"The predicted singer for {mp3_file} is: {predicted_singer}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {mp3_file}: {str(e)}\")\n",
        "\n",
        "print(\"Classification¬†complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "681MxBv728Hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd20cd50-9a4d-4484-9c42-0b62c67700a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Expected 9 MP3 files, but found 0 files.\n",
            "Classification¬†complete.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Load your trained model and feature extractor\n",
        "model_name = \"./trained_audio_classifier\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "# Load the label mapping\n",
        "label_map_path = os.path.join(model_name, \"label_map.json\")\n",
        "with open(label_map_path, \"r\") as f:\n",
        "    id_to_label = json.load(f)\n",
        "\n",
        "# Function to preprocess and predict\n",
        "def predict_singer(audio_file_path):\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "    # Resample if necessary (assuming your model expects 16kHz)\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Convert to mono if stereo\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Preprocess the audio\n",
        "    inputs = feature_extractor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    # Get the predicted class\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    predicted_singer = id_to_label[str(predicted_class_id)]\n",
        "\n",
        "    return predicted_singer\n",
        "\n",
        "# Directory containing the input MP3 files\n",
        "input_directory = \"/content/drive/MyDrive/INPUT\"\n",
        "\n",
        "# Get all MP3 files in the input directory\n",
        "mp3_files = [f for f in os.listdir(input_directory) if f.endswith('.mp3')]\n",
        "\n",
        "# Ensure we have exactly 9 MP3 files\n",
        "if len(mp3_files) != 9:\n",
        "    print(f\"Warning: Expected 9 MP3 files, but found {len(mp3_files)} files.\")\n",
        "\n",
        "# Process each MP3 file\n",
        "for mp3_file in mp3_files:\n",
        "    audio_file_path = os.path.join(input_directory, mp3_file)\n",
        "    try:\n",
        "        predicted_singer = predict_singer(audio_file_path)\n",
        "        print(f\"The predicted singer for {mp3_file} is: {predicted_singer}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {mp3_file}: {str(e)}\")\n",
        "\n",
        "print(\"Classification¬†complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Licfg8SX3B7C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2bf3daac-0ead-4561-8e45-5ab16fe7ddca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to open the input \"/content/drive/MyDrive/INPUT/sg3.mp3\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7d0fcc578897 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7d0fcc528b25 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42334 (0x7d0ff118d334 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7d0ff118fd34 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3aa4e (0x7d0ccf8e1a4e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32617 (0x7d0ccf8d9617 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15adae (0x5c636f177dae in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x5c636f16e52b in /usr/bin/python3)\nframe #8: <unknown function> + 0x169680 (0x5c636f186680 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165dc7 (0x5c636f182dc7 in /usr/bin/python3)\nframe #10: <unknown function> + 0x1518db (0x5c636f16e8db in /usr/bin/python3)\nframe #11: <unknown function> + 0xf6cb (0x7d0ff11d16cb in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x5c636f16e52b in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6f0b (0x5c636f16716b in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x5c636f16d6c4 in /usr/bin/python3)\nframe #15: <unknown function> + 0x1657a4 (0x5c636f1827a4 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x5c636f16e4cc in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6f0b (0x5c636f16716b in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x64e2 (0x5c636f166742 in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x64e2 (0x5c636f166742 in /usr/bin/python3)\nframe #24: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #26: <unknown function> + 0x140096 (0x5c636f15d096 in /usr/bin/python3)\nframe #27: PyEval_EvalCode + 0x86 (0x5c636f252f66 in /usr/bin/python3)\nframe #28: <unknown function> + 0x23ba2d (0x5c636f258a2d in /usr/bin/python3)\nframe #29: <unknown function> + 0x15b909 (0x5c636f178909 in /usr/bin/python3)\nframe #30: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #31: <unknown function> + 0x1788d0 (0x5c636f1958d0 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x2891 (0x5c636f162af1 in /usr/bin/python3)\nframe #33: <unknown function> + 0x1788d0 (0x5c636f1958d0 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x2891 (0x5c636f162af1 in /usr/bin/python3)\nframe #35: <unknown function> + 0x1788d0 (0x5c636f1958d0 in /usr/bin/python3)\nframe #36: <unknown function> + 0x2566ff (0x5c636f2736ff in /usr/bin/python3)\nframe #37: <unknown function> + 0x16700a (0x5c636f18400a in /usr/bin/python3)\nframe #38: _PyEval_EvalFrameDefault + 0x8cb (0x5c636f160b2b in /usr/bin/python3)\nframe #39: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x8cb (0x5c636f160b2b in /usr/bin/python3)\nframe #43: <unknown function> + 0x169251 (0x5c636f186251 in /usr/bin/python3)\nframe #44: PyObject_Call + 0x122 (0x5c636f186f02 in /usr/bin/python3)\nframe #45: _PyEval_EvalFrameDefault + 0x2a49 (0x5c636f162ca9 in /usr/bin/python3)\nframe #46: <unknown function> + 0x169251 (0x5c636f186251 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x19b6 (0x5c636f161c16 in /usr/bin/python3)\nframe #48: <unknown function> + 0x201a15 (0x5c636f21ea15 in /usr/bin/python3)\nframe #49: <unknown function> + 0x15b909 (0x5c636f178909 in /usr/bin/python3)\nframe #50: <unknown function> + 0x2378e5 (0x5c636f2548e5 in /usr/bin/python3)\nframe #51: <unknown function> + 0x2b42c2 (0x5c636f2d12c2 in /usr/bin/python3)\nframe #52: <unknown function> + 0x14e44b (0x5c636f16b44b in /usr/bin/python3)\nframe #53: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #54: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x8cb (0x5c636f160b2b in /usr/bin/python3)\nframe #56: <unknown function> + 0x201a15 (0x5c636f21ea15 in /usr/bin/python3)\nframe #57: <unknown function> + 0x15b909 (0x5c636f178909 in /usr/bin/python3)\nframe #58: <unknown function> + 0x2378e5 (0x5c636f2548e5 in /usr/bin/python3)\nframe #59: <unknown function> + 0x2b42c2 (0x5c636f2d12c2 in /usr/bin/python3)\nframe #60: <unknown function> + 0x14e44b (0x5c636f16b44b in /usr/bin/python3)\nframe #61: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #62: <unknown function> + 0x169251 (0x5c636f186251 in /usr/bin/python3)\nframe #63: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a947ab880945>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Use the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0maudio_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/INPUT/sg3.mp3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mpredicted_singer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_singer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The predicted singer is: {predicted_singer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-a947ab880945>\u001b[0m in \u001b[0;36mpredict_singer\u001b[0;34m(audio_file_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_singer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Load the audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Resample if necessary (assuming your model expects 16kHz)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"/content/drive/MyDrive/INPUT/sg3.mp3\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7d0fcc578897 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7d0fcc528b25 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42334 (0x7d0ff118d334 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7d0ff118fd34 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3aa4e (0x7d0ccf8e1a4e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32617 (0x7d0ccf8d9617 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15adae (0x5c636f177dae in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x5c636f16e52b in /usr/bin/python3)\nframe #8: <unknown function> + 0x169680 (0x5c636f186680 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165dc7 (0x5c636f182dc7 in /usr/bin/python3)\nframe #10: <unknown function> + 0x1518db (0x5c636f16e8db in /usr/bin/python3)\nframe #11: <unknown function> + 0xf6cb (0x7d0ff11d16cb in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x5c636f16e52b in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6f0b (0x5c636f16716b in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x5c636f16d6c4 in /usr/bin/python3)\nframe #15: <unknown function> + 0x1657a4 (0x5c636f1827a4 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x5c636f16e4cc in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6f0b (0x5c636f16716b in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x64e2 (0x5c636f166742 in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x64e2 (0x5c636f166742 in /usr/bin/python3)\nframe #24: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #26: <unknown function> + 0x140096 (0x5c636f15d096 in /usr/bin/python3)\nframe #27: PyEval_EvalCode + 0x86 (0x5c636f252f66 in /usr/bin/python3)\nframe #28: <unknown function> + 0x23ba2d (0x5c636f258a2d in /usr/bin/python3)\nframe #29: <unknown function> + 0x15b909 (0x5c636f178909 in /usr/bin/python3)\nframe #30: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #31: <unknown function> + 0x1788d0 (0x5c636f1958d0 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x2891 (0x5c636f162af1 in /usr/bin/python3)\nframe #33: <unknown function> + 0x1788d0 (0x5c636f1958d0 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x2891 (0x5c636f162af1 in /usr/bin/python3)\nframe #35: <unknown function> + 0x1788d0 (0x5c636f1958d0 in /usr/bin/python3)\nframe #36: <unknown function> + 0x2566ff (0x5c636f2736ff in /usr/bin/python3)\nframe #37: <unknown function> + 0x16700a (0x5c636f18400a in /usr/bin/python3)\nframe #38: _PyEval_EvalFrameDefault + 0x8cb (0x5c636f160b2b in /usr/bin/python3)\nframe #39: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x8cb (0x5c636f160b2b in /usr/bin/python3)\nframe #43: <unknown function> + 0x169251 (0x5c636f186251 in /usr/bin/python3)\nframe #44: PyObject_Call + 0x122 (0x5c636f186f02 in /usr/bin/python3)\nframe #45: _PyEval_EvalFrameDefault + 0x2a49 (0x5c636f162ca9 in /usr/bin/python3)\nframe #46: <unknown function> + 0x169251 (0x5c636f186251 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x19b6 (0x5c636f161c16 in /usr/bin/python3)\nframe #48: <unknown function> + 0x201a15 (0x5c636f21ea15 in /usr/bin/python3)\nframe #49: <unknown function> + 0x15b909 (0x5c636f178909 in /usr/bin/python3)\nframe #50: <unknown function> + 0x2378e5 (0x5c636f2548e5 in /usr/bin/python3)\nframe #51: <unknown function> + 0x2b42c2 (0x5c636f2d12c2 in /usr/bin/python3)\nframe #52: <unknown function> + 0x14e44b (0x5c636f16b44b in /usr/bin/python3)\nframe #53: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #54: _PyFunction_Vectorcall + 0x7c (0x5c636f1786ac in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x8cb (0x5c636f160b2b in /usr/bin/python3)\nframe #56: <unknown function> + 0x201a15 (0x5c636f21ea15 in /usr/bin/python3)\nframe #57: <unknown function> + 0x15b909 (0x5c636f178909 in /usr/bin/python3)\nframe #58: <unknown function> + 0x2378e5 (0x5c636f2548e5 in /usr/bin/python3)\nframe #59: <unknown function> + 0x2b42c2 (0x5c636f2d12c2 in /usr/bin/python3)\nframe #60: <unknown function> + 0x14e44b (0x5c636f16b44b in /usr/bin/python3)\nframe #61: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\nframe #62: <unknown function> + 0x169251 (0x5c636f186251 in /usr/bin/python3)\nframe #63: _PyEval_EvalFrameDefault + 0x6d5 (0x5c636f160935 in /usr/bin/python3)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "\n",
        "# Load your trained model and feature extractor\n",
        "model_name = \"./trained_audio_classifier\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "# Function to preprocess and predict\n",
        "def predict_singer(audio_file_path):\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "    # Resample if necessary (assuming your model expects 16kHz)\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Convert to mono if stereo\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Preprocess the audio\n",
        "    inputs = feature_extractor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\")\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "# Map the class ID to singer name (you'll need to create this mapping)\n",
        "    singer_names = {0:\"SP BALASUBRAMANYUM\", 1:\"SHANKAR MAHADEVAN\", 2:\"SHREYA GHOSHAL\"}  # Replace with your actual singer names\n",
        "    predicted_singer = singer_names[predicted_class_id]\n",
        "\n",
        "    return predicted_singer\n",
        "\n",
        "# Use the function\n",
        "audio_file_path = \"/content/drive/MyDrive/INPUT/sg3.mp3\"\n",
        "predicted_singer = predict_singer(audio_file_path)\n",
        "print(f\"The predicted singer is: {predicted_singer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzTpBSSC3o70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3d20d2-74d6-46db-828b-1d7fcdab7230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 0 audio files:\n",
            "\n",
            "Prediction completed for all¬†audio¬†files.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the trained model and feature extractor\n",
        "model_name = \"./trained_audio_classifier\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def predict_singer(audio_file_path, confidence_threshold=0.5):\n",
        "    try:\n",
        "        # Load the audio file\n",
        "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "        # Resample if necessary (assuming 16kHz is required)\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Convert to mono if stereo\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Normalize\n",
        "        waveform = waveform / torch.max(torch.abs(waveform))\n",
        "\n",
        "        # Preprocess the audio\n",
        "        inputs = feature_extractor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Get the predicted class and its probability\n",
        "        predicted_class_id = logits.argmax().item()\n",
        "        predicted_prob = probs[0][predicted_class_id].item()\n",
        "\n",
        "        # Map the class ID to singer name\n",
        "        id_to_label = {0: \"SP BALASUBRAMANYUM\", 1: \"SHANKAR MAHADEVAN\", 2: \"SHREYA GHOSHAL\"}\n",
        "        predicted_singer = id_to_label[predicted_class_id]\n",
        "\n",
        "        # Print probabilities for all classes\n",
        "        print(f\"Probabilities for {os.path.basename(audio_file_path)}:\")\n",
        "        for i, prob in enumerate(probs[0]):\n",
        "            print(f\"{id_to_label[i]}: {prob.item():.4f}\")\n",
        "\n",
        "        # Check if the prediction meets the confidence threshold\n",
        "        if predicted_prob >= confidence_threshold:\n",
        "            return predicted_singer, predicted_prob\n",
        "        else:\n",
        "            return \"Uncertain\", predicted_prob\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_file_path}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# Process all audio files in the input directory\n",
        "input_directory = \"/content/drive/MyDrive/INPUT\"\n",
        "\n",
        "# List to store valid audio files\n",
        "valid_audio_files = []\n",
        "\n",
        "# Collect valid audio files\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith((\".mp3\", \".wav\", \".ogg\")):  # Add or remove file extensions as needed\n",
        "        valid_audio_files.append(filename)\n",
        "\n",
        "# Ensure we process only 9 files\n",
        "num_files_to_process = min(9, len(valid_audio_files))\n",
        "\n",
        "print(f\"Processing {num_files_to_process} audio files:\")\n",
        "\n",
        "for i in range(num_files_to_process):\n",
        "    filename = valid_audio_files[i]\n",
        "    audio_file_path = os.path.join(input_directory, filename)\n",
        "    predicted_singer, confidence = predict_singer(audio_file_path)\n",
        "    if predicted_singer:\n",
        "        print(f\"The predicted singer for {filename} is: {predicted_singer} (Confidence: {confidence:.4f})\")\n",
        "    print()  # Add a blank line for readability\n",
        "\n",
        "print(\"\\nPrediction completed for all¬†audio¬†files.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9c489ebda9194633bb19e66e4018a57e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b58b0eb12c1f443dbed654fe8d734c70",
              "IPY_MODEL_362ace8f10e144eb90c574111bd2106d",
              "IPY_MODEL_6d7430f37f304fa2be532f9a09c6aadf"
            ],
            "layout": "IPY_MODEL_1c6b3e2fdc244e22b55265fb622004f2"
          }
        },
        "b58b0eb12c1f443dbed654fe8d734c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4e0c09e42a14cad93e8f91554d4f7b7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_edceff8ff4494a13bbc3c74fecd73f6a",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "362ace8f10e144eb90c574111bd2106d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09616d5fdf954ca4af87dfb4d612db09",
            "max": 159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_235593f5380b4ac7afc43949b0b0af8a",
            "value": 159
          }
        },
        "6d7430f37f304fa2be532f9a09c6aadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4db3f96dd914460899491123f8066ca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7be18603adfe4fdfa0feb4c5d6d502b5",
            "value": "‚Äá159/159‚Äá[00:00&lt;00:00,‚Äá9.71kB/s]"
          }
        },
        "1c6b3e2fdc244e22b55265fb622004f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4e0c09e42a14cad93e8f91554d4f7b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edceff8ff4494a13bbc3c74fecd73f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09616d5fdf954ca4af87dfb4d612db09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "235593f5380b4ac7afc43949b0b0af8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4db3f96dd914460899491123f8066ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7be18603adfe4fdfa0feb4c5d6d502b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b702b3dae7e14505a0a4d285e7dea0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ba797f8005742b2a7334d50afe0e11a",
              "IPY_MODEL_37f5a6639e2e496297b98f011ac22089",
              "IPY_MODEL_9871f60daec84b04a3774684f2723c77"
            ],
            "layout": "IPY_MODEL_76b46386841941bab0e7a5b4baacdf74"
          }
        },
        "9ba797f8005742b2a7334d50afe0e11a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db045aa0e872404584e5a6a746f11344",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6357fa642afe41ed96894c5a156e09da",
            "value": "config.json:‚Äá100%"
          }
        },
        "37f5a6639e2e496297b98f011ac22089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4652ac5ea67b44e181e66bbbd8bd7b1b",
            "max": 1842,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a563cb9bf11c456bbea6d57979bbc54a",
            "value": 1842
          }
        },
        "9871f60daec84b04a3774684f2723c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e8fd1b6b6644eb78f88ba66c733b524",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9edcb389d90c4d2a97bbb28e3c176c15",
            "value": "‚Äá1.84k/1.84k‚Äá[00:00&lt;00:00,‚Äá148kB/s]"
          }
        },
        "76b46386841941bab0e7a5b4baacdf74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db045aa0e872404584e5a6a746f11344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6357fa642afe41ed96894c5a156e09da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4652ac5ea67b44e181e66bbbd8bd7b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a563cb9bf11c456bbea6d57979bbc54a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e8fd1b6b6644eb78f88ba66c733b524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9edcb389d90c4d2a97bbb28e3c176c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a03a68036944285a67d4be98d57149c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f977f66658b64094a218bceb327d246d",
              "IPY_MODEL_3d787af8d766451591d83db4dbf63ce3",
              "IPY_MODEL_62ad80147f6f43bf9a6592027db827d0"
            ],
            "layout": "IPY_MODEL_16e822cb96fb44afb71b83fe210eb388"
          }
        },
        "f977f66658b64094a218bceb327d246d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9676fc96641740a0ae385f384e19bea0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_520fda3ad70f4d16bb90e41338139843",
            "value": "pytorch_model.bin:‚Äá100%"
          }
        },
        "3d787af8d766451591d83db4dbf63ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40676458d90b4b56890904e59b8c3661",
            "max": 380267417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4360b3e286f54fccb119e7dcf908f405",
            "value": 380267417
          }
        },
        "62ad80147f6f43bf9a6592027db827d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce24995d61b4445082ab2cb56dbc4d8d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_eb4d1bd56ca34cafa45bda1f058cd169",
            "value": "‚Äá380M/380M‚Äá[00:01&lt;00:00,‚Äá262MB/s]"
          }
        },
        "16e822cb96fb44afb71b83fe210eb388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9676fc96641740a0ae385f384e19bea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "520fda3ad70f4d16bb90e41338139843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40676458d90b4b56890904e59b8c3661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4360b3e286f54fccb119e7dcf908f405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce24995d61b4445082ab2cb56dbc4d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb4d1bd56ca34cafa45bda1f058cd169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}